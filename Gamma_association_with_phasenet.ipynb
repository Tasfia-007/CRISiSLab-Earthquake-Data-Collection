{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVl7TokFbVgsWNw6PuF77j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tasfia-007/CRISiSLab-Earthquake-Data-Collection/blob/main/Gamma_association_with_phasenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install obspy seisbench pyproj seaborn\n",
        "!pip install git+https://github.com/wayneweiqiang/GaMMA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NUKCcIBgYtmi",
        "outputId": "a3993533-93c5-44e2-cd49-44509bc50a8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting obspy\n",
            "  Downloading obspy-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting seisbench\n",
            "  Downloading seisbench-0.10.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pyproj in /usr/local/lib/python3.12/dist-packages (3.7.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from obspy) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from obspy) (1.16.3)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.12/dist-packages (from obspy) (3.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from obspy) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from obspy) (75.2.0)\n",
            "Collecting sqlalchemy<2 (from obspy)\n",
            "  Downloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from obspy) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from obspy) (2.32.4)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.12/dist-packages (from seisbench) (2.2.2)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.12/dist-packages (from seisbench) (3.15.1)\n",
            "Requirement already satisfied: tqdm>=4.52 in /usr/local/lib/python3.12/dist-packages (from seisbench) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from seisbench) (2.8.0+cu126)\n",
            "Requirement already satisfied: nest_asyncio>=1.5.3 in /usr/local/lib/python3.12/dist-packages (from seisbench) (1.6.0)\n",
            "Requirement already satisfied: bottleneck>=1.3 in /usr/local/lib/python3.12/dist-packages (from seisbench) (1.4.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyproj) (2025.10.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->obspy) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1->seisbench) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1->seisbench) (2025.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<2->obspy) (3.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->seisbench) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->obspy) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->obspy) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->seisbench) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->seisbench) (3.0.3)\n",
            "Downloading obspy-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seisbench-0.10.2-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlalchemy, obspy, seisbench\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.44\n",
            "    Uninstalling SQLAlchemy-2.0.44:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.44\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
            "google-adk 1.17.0 requires sqlalchemy<3.0.0,>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed obspy-1.4.2 seisbench-0.10.2 sqlalchemy-1.4.54\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "signal"
                ]
              },
              "id": "18c6b372dff040158f98ad3443e80be7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/wayneweiqiang/GaMMA.git\n",
            "  Cloning https://github.com/wayneweiqiang/GaMMA.git to /tmp/pip-req-build-onb0yi1i\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/wayneweiqiang/GaMMA.git /tmp/pip-req-build-onb0yi1i\n",
            "  Resolved https://github.com/wayneweiqiang/GaMMA.git to commit f6b1ac7680f50bcc7d5d3928361ba02a7df0f523\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==1.7 (from GMMA==1.2.17)\n",
            "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from GMMA==1.2.17) (1.16.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from GMMA==1.2.17) (2.0.2)\n",
            "Requirement already satisfied: pyproj in /usr/local/lib/python3.12/dist-packages (from GMMA==1.2.17) (3.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from GMMA==1.2.17) (4.67.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from GMMA==1.2.17) (0.60.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.7->GMMA==1.2.17) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.7->GMMA==1.2.17) (3.6.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->GMMA==1.2.17) (0.43.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyproj->GMMA==1.2.17) (2025.10.5)\n",
            "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: GMMA\n",
            "  Building wheel for GMMA (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GMMA: filename=GMMA-1.2.17-py3-none-any.whl size=32837 sha256=f493fd9760087d2efdb39223e2f008192f149fa013adc8dc954d23b3cfae1082\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bxioarmr/wheels/97/45/22/9a91feb083fef5b93b0c4c52beb49d057eb7ae3c7eea92a657\n",
            "Successfully built GMMA\n",
            "Installing collected packages: scikit-learn, GMMA\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed GMMA-1.2.17 scikit-learn-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seismic Event Detection using Windowed GAMMA Association\n",
        "\n",
        "## Overview\n",
        "This code implements earthquake event detection from seismic phase picks using the GAMMA (Gaussian Mixture Model Association) algorithm. It processes PhaseNet-generated P-wave and S-wave picks through time-windowed analysis with velocity grid search optimization.\n",
        "\n",
        "---\n",
        "\n",
        "## Problem Statement & Initial Fix\n",
        "\n",
        "### Critical Station ID Mismatch\n",
        "The first major issue was that pick IDs didn't match station codes in the station dataframe. Pick IDs came in format `NETWORK.STATION.LOCATION.CHANNEL` (e.g., `NZ.WEL.10.HHZ`), while the station dataframe only had station codes.\n",
        "\n",
        "**Solution:**\n",
        "```python\n",
        "# Extract middle component (station code) from pick IDs\n",
        "pick_df['id'] = pick_df['id'].str.split('.').str[1]\n",
        "```\n",
        "\n",
        "This extraction achieved 100% match rate - critical because without matching station IDs, we can't retrieve station coordinates needed for earthquake location.\n",
        "\n",
        "---\n",
        "\n",
        "## Coordinate System Transformation\n",
        "\n",
        "Since we're working with New Zealand data, I converted from WGS84 (lat/lon) to NZTM projection (EPSG:2193) - a local Cartesian coordinate system in kilometers.\n",
        "\n",
        "```python\n",
        "transformer = Transformer.from_crs(wgs84, local_crs)\n",
        "station_df[\"x(km)\"] = ...  # Easting coordinate\n",
        "station_df[\"y(km)\"] = ...  # Northing coordinate  \n",
        "station_df[\"z(km)\"] = -station_df[\"depth_km\"]  # Negative because depth is downward\n",
        "```\n",
        "\n",
        "**Why this matters:** GAMMA requires distances in km for travel-time calculations. Lat/lon degrees don't work directly for distance calculations.\n",
        "\n",
        "---\n",
        "\n",
        "## Time Windowing Strategy\n",
        "\n",
        "### Why 15-Minute Windows?\n",
        "Processing the entire dataset at once is computationally prohibitive. The data spans ~180 hours, so I implemented a sliding window approach:\n",
        "\n",
        "- **Window size:** 15 minutes\n",
        "- **Minimum picks per window:** 2 (below this, association is meaningless)\n",
        "- **Non-overlapping windows:** Sequential processing\n",
        "\n",
        "**Window Creation Logic:**\n",
        "```python\n",
        "current_time = start_time\n",
        "while current_time < end_time:\n",
        "    window_end = current_time + timedelta(minutes=WINDOW_SIZE)\n",
        "    mask = (pick_df['timestamp'] >= current_time) & (pick_df['timestamp'] < window_end)\n",
        "    if mask.sum() >= MIN_PICKS:\n",
        "        windows.append({...})\n",
        "    current_time = window_end\n",
        "```\n",
        "\n",
        "This reduces memory usage and allows parallel processing (though not implemented here).\n",
        "\n",
        "---\n",
        "\n",
        "## Velocity Grid Search - The Core Innovation\n",
        "\n",
        "### The Velocity Problem\n",
        "Earthquake location accuracy critically depends on seismic wave velocities (Vp for P-waves, Vs for S-waves). But we don't know the exact velocity structure beforehand!\n",
        "\n",
        "### Solution: Exhaustive Grid Search\n",
        "multiple velocity models(ak-125f) and select the one that produces the most coherent events:\n",
        "\n",
        "```python\n",
        "vel_p_values = [1.45, 1.65, 5.8, 6.8, 8]      # km/s - P-wave velocities\n",
        "vel_s_values = [1.0, 3.2, 3.9, 4.48, 4.49, 4.5]  # km/s - S-wave velocities\n",
        "```\n",
        "\n",
        "**Velocity ranges explained:**\n",
        "- **1.45-1.65 km/s:** Shallow sedimentary layers, water-saturated zones\n",
        "- **5.8-6.8 km/s (Vp):** Typical continental crust velocities\n",
        "- **8.0 km/s (Vp):** Lower crust / upper mantle\n",
        "- **3.2-4.5 km/s (Vs):** Corresponding S-wave velocities\n",
        "\n",
        "**Physical constraint:** Always `Vs < Vp` (S-waves are inherently slower than P-waves)\n",
        "\n",
        "### Grid Search Implementation\n",
        "For each time window:\n",
        "```python\n",
        "for vp in vel_p_values:\n",
        "    for vs in vel_s_values:\n",
        "        if vs >= vp:  # Invalid combination - skip\n",
        "            continue\n",
        "        \n",
        "        config[\"vel\"] = {\"p\": vp, \"s\": vs}\n",
        "        catalogs, assignments = association(window_picks, station_df, config)\n",
        "        n_events = len(catalogs)\n",
        "        \n",
        "        # Keep the velocity model that detects the MOST events\n",
        "        if n_events > best_n_events:\n",
        "            best_n_events = n_events\n",
        "            best_catalog = catalogs\n",
        "            best_params = {'vp': vp, 'vs': vs}\n",
        "```\n",
        "\n",
        "**Rationale:** The velocity model that produces the maximum number of well-constrained events is most likely correct for that region and time period. This is an implicit optimization criterion.\n",
        "\n",
        "---\n",
        "\n",
        "## GAMMA Configuration Parameters\n",
        "\n",
        "### Spatial Search Bounds\n",
        "Extended 20 km beyond the station network:\n",
        "```python\n",
        "\"x(km)\": (x_min - 20, x_max + 20)\n",
        "\"y(km)\": (y_min - 20, y_max + 20)  \n",
        "\"z(km)\": (0, max(200, z_max + 20))\n",
        "```\n",
        "The 20 km buffer accounts for events outside the network that can still be detected by distant stations.\n",
        "\n",
        "### DBSCAN Clustering Parameters\n",
        "GAMMA uses a two-stage approach:\n",
        "1. **BGMM (Bayesian Gaussian Mixture Model):** Generates candidate event locations\n",
        "2. **DBSCAN:** Clusters picks in time-space to identify individual events\n",
        "\n",
        "```python\n",
        "\"dbscan_eps\": 10,              # seconds - temporal clustering threshold\n",
        "\"dbscan_min_samples\": 2,       # minimum picks to form a cluster\n",
        "\"min_picks_per_eq\": 2,         # minimum picks required per event\n",
        "```\n",
        "\n",
        "**Why these values:**\n",
        "- `eps=10s`: Events separated by >10s are distinct\n",
        "- `min_samples=2`: Conservative - need at least 2 picks to define an event\n",
        "- This minimizes false positives at the cost of potentially missing small events\n",
        "\n",
        "### Method Selection\n",
        "```python\n",
        "\"method\": \"BGMM\"              # Bayesian Gaussian Mixture Model\n",
        "\"oversample_factor\": 4         # 4x density of candidate points\n",
        "```\n",
        "BGMM is preferred over standard GMM for sparse data because it handles varying cluster sizes better and has built-in regularization.\n",
        "\n",
        "### Uncertainty Constraints\n",
        "```python\n",
        "\"max_sigma11\": 3.0,   # Maximum uncertainty in x-direction (km)\n",
        "\"max_sigma22\": 1.5,   # Maximum uncertainty in y-direction (km)\n",
        "\"max_sigma12\": 1.5,   # Maximum covariance between x-y\n",
        "```\n",
        "Events with uncertainties exceeding these thresholds are automatically filtered out. This ensures we only keep well-constrained locations.\n",
        "\n",
        "### BFGS Optimization Bounds\n",
        "```python\n",
        "\"bfgs_bounds\": (\n",
        "    (x_min - 1, x_max + 1),  # Allow slight extension beyond spatial bounds\n",
        "    (y_min - 1, y_max + 1),\n",
        "    (0, z_max + 1),           # Depth must be positive\n",
        "    (None, None),             # Unconstrained origin time\n",
        ")\n",
        "```\n",
        "GAMMA uses BFGS optimization to refine event locations. These bounds prevent the optimizer from wandering into physically unrealistic regions.\n",
        "\n",
        "---\n",
        "\n",
        "## Output Files\n",
        "\n",
        "### 1. `catalog_windowed(15).csv`\n",
        "Contains detected earthquake events with:\n",
        "- **x, y, z:** Event location in NZTM coordinates (km)\n",
        "- **time:** Origin time of the earthquake\n",
        "- **sigma11, sigma22, sigma12:** Location uncertainties and covariances\n",
        "- **gamma_score:** Association quality metric (0-1, higher is better)\n",
        "- **window_idx, window_start:** Metadata about which time window detected this event\n",
        "\n",
        "### 2. `assignments_windowed(15).csv`\n",
        "Pick-to-event associations:\n",
        "- **pick_index:** Index of the original pick in the input dataframe\n",
        "- **event_index:** Which event this pick is assigned to\n",
        "- **gamma_score:** Confidence of this specific association\n",
        "- **window_idx:** Which time window produced this association\n",
        "\n",
        "This file is crucial for quality control - you can trace back which picks contributed to each event.\n",
        "\n",
        "### 3. `window_results(15).csv`\n",
        "Summary statistics per time window:\n",
        "- **n_events:** Number of events detected in this window\n",
        "- **n_picks:** Total picks available in this window\n",
        "- **vp, vs:** Best velocity model for this window\n",
        "- **window_start:** Window timestamp\n",
        "\n",
        "This provides insights into temporal variations in seismicity and velocity structure.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "nQNnPtXEYxqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "3U7HFC3KX_bJ",
        "outputId": "9589a922-5ef4-41d7-90b8-30d49d3e830e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gamma'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4241833520.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massociation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gamma'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gamma.utils import association\n",
        "from itertools import product\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load data\n",
        "print(\"Loading picks and stations...\")\n",
        "pick_df = pd.read_csv(\"/kaggle/working/phasenet_picks.csv\")\n",
        "station_csv = \"/kaggle/working/available_waveform_summary.csv\"\n",
        "station_df = pd.read_csv(station_csv)\n",
        "\n",
        "# ============================================================\n",
        "# FIX: Extract station code from pick IDs\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FIXING STATION ID MISMATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nBefore fix:\")\n",
        "print(f\"  Sample pick IDs: {pick_df['id'].iloc[:3].tolist()}\")\n",
        "print(f\"  Sample station IDs: {station_df['station_code'].iloc[:3].tolist()}\")\n",
        "\n",
        "# Extract station code (middle part between dots)\n",
        "pick_df['id'] = pick_df['id'].str.split('.').str[1]\n",
        "\n",
        "print(f\"\\nAfter fix:\")\n",
        "print(f\"  Sample pick IDs: {pick_df['id'].iloc[:3].tolist()}\")\n",
        "\n",
        "# Verify matches\n",
        "matches = pick_df['id'].isin(station_df['station_code']).sum()\n",
        "print(f\"\\n✓ Matching stations: {matches}/{len(pick_df)} picks ({100*matches/len(pick_df):.1f}%)\")\n",
        "\n",
        "if matches == 0:\n",
        "    print(\"\\n⚠ ERROR: Still no matches! Check station_code column name\")\n",
        "    print(f\"Station dataframe columns: {station_df.columns.tolist()}\")\n",
        "    exit()\n",
        "\n",
        "# ============================================================\n",
        "# Setup coordinate transformation\n",
        "# ============================================================\n",
        "from pyproj import CRS, Transformer\n",
        "wgs84 = CRS.from_epsg(4326)\n",
        "local_crs = CRS.from_epsg(2193) #for nz\n",
        "transformer = Transformer.from_crs(wgs84, local_crs)\n",
        "\n",
        "station_df[\"x(km)\"] = station_df.apply(\n",
        "    lambda row: transformer.transform(row[\"station_latitude\"], row[\"station_longitude\"])[0] / 1000, axis=1\n",
        ")\n",
        "station_df[\"y(km)\"] = station_df.apply(\n",
        "    lambda row: transformer.transform(row[\"station_latitude\"], row[\"station_longitude\"])[1] / 1000, axis=1\n",
        ")\n",
        "station_df[\"z(km)\"] = -station_df[\"depth_km\"]\n",
        "station_df[\"id\"] = station_df[\"station_code\"]\n",
        "station_df = station_df.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
        "\n",
        "# Set bounding box\n",
        "x_min, x_max = station_df[\"x(km)\"].min(), station_df[\"x(km)\"].max()\n",
        "y_min, y_max = station_df[\"y(km)\"].min(), station_df[\"y(km)\"].max()\n",
        "z_min, z_max = station_df[\"z(km)\"].min(), station_df[\"z(km)\"].max()\n",
        "\n",
        "print(f\"\\nData Summary:\")\n",
        "print(f\"Total picks: {len(pick_df)}\")\n",
        "print(f\"P picks: {sum(pick_df['type'] == 'p')}\")\n",
        "print(f\"S picks: {sum(pick_df['type'] == 's')}\")\n",
        "print(f\"Unique stations: {len(station_df)}\")\n",
        "print(f\"X range: ({x_min:.1f}, {x_max:.1f}) km\")\n",
        "print(f\"Y range: ({y_min:.1f}, {y_max:.1f}) km\")\n",
        "print(f\"Z range: ({z_min:.1f}, {z_max:.1f}) km\")\n",
        "\n",
        "# Convert timestamps\n",
        "pick_df['timestamp'] = pd.to_datetime(pick_df['timestamp'])\n",
        "time_span = (pick_df['timestamp'].max() - pick_df['timestamp'].min()).total_seconds()\n",
        "print(f\"Time span of picks: {time_span:.1f} seconds ({time_span/3600:.1f} hours)\")\n",
        "\n",
        "# ============================================================\n",
        "# 15-MINUTE WINDOW PROCESSING WITH VELOCITY GRID SEARCH\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROCESSING IN 30-MINUTE WINDOWS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Fixed parameters\n",
        "WINDOW_SIZE = 15 # minutes   1500*180 merged divide 15 minutes\n",
        "MIN_PICKS = 2\n",
        "DBSCAN_EPS = 10\n",
        "DBSCAN_MIN_SAMPLES = 2\n",
        "MIN_PICKS_PER_EQ = 2\n",
        "\n",
        "# Velocity grid (reduced for efficiency)\n",
        "vel_p_values = [1.45, 1.65, 5.8, 6.8, 8]\n",
        "vel_s_values = [1.0, 3.2, 3.9, 4.48, 4.49, 4.5]\n",
        "\n",
        "print(f\"\\nFixed parameters:\")\n",
        "print(f\"  Window size: {WINDOW_SIZE} minutes\")\n",
        "print(f\"  Min picks per window: {MIN_PICKS}\")\n",
        "print(f\"  DBSCAN eps: {DBSCAN_EPS} seconds\")\n",
        "print(f\"  DBSCAN min_samples: {DBSCAN_MIN_SAMPLES}\")\n",
        "print(f\"  Min picks per event: {MIN_PICKS_PER_EQ}\")\n",
        "\n",
        "print(f\"\\nVelocity grid:\")\n",
        "print(f\"  Vp values: {vel_p_values}\")\n",
        "print(f\"  Vs values: {vel_s_values}\")\n",
        "print(f\"  Total combinations: {len(vel_p_values) * len(vel_s_values)}\")\n",
        "\n",
        "# Create time windows\n",
        "start_time = pick_df['timestamp'].min()\n",
        "end_time = pick_df['timestamp'].max()\n",
        "current_time = start_time\n",
        "\n",
        "windows = []\n",
        "while current_time < end_time:\n",
        "    window_end = current_time + timedelta(minutes=WINDOW_SIZE)\n",
        "    mask = (pick_df['timestamp'] >= current_time) & (pick_df['timestamp'] < window_end)\n",
        "    n_picks = mask.sum()\n",
        "    if n_picks >= MIN_PICKS:\n",
        "        windows.append({\n",
        "            'start': current_time,\n",
        "            'end': window_end,\n",
        "            'n_picks': n_picks\n",
        "        })\n",
        "    current_time = window_end\n",
        "\n",
        "print(f\"\\nTotal windows: {len(windows)}\")\n",
        "print(f\"Windows with ≥{MIN_PICKS} picks: {len(windows)}\")\n",
        "\n",
        "# Base configuration\n",
        "base_config = {\n",
        "    \"dims\": ['x(km)', 'y(km)', 'z(km)'],\n",
        "    \"use_dbscan\": True,\n",
        "    \"use_amplitude\": False,\n",
        "    \"x(km)\": (x_min - 20, x_max + 20),\n",
        "    \"y(km)\": (y_min - 20, y_max + 20),\n",
        "    \"z(km)\": (0, max(200, z_max + 20)),\n",
        "    \"method\": \"BGMM\",\n",
        "    \"oversample_factor\": 4,\n",
        "    \"dbscan_eps\": DBSCAN_EPS,\n",
        "    \"dbscan_min_samples\": DBSCAN_MIN_SAMPLES,\n",
        "    \"min_picks_per_eq\": MIN_PICKS_PER_EQ,\n",
        "    \"max_sigma11\": 3.0,\n",
        "    \"max_sigma22\": 1.5,\n",
        "    \"max_sigma12\": 1.5,\n",
        "}\n",
        "\n",
        "base_config[\"bfgs_bounds\"] = (\n",
        "    (base_config[\"x(km)\"][0] - 1, base_config[\"x(km)\"][1] + 1),\n",
        "    (base_config[\"y(km)\"][0] - 1, base_config[\"y(km)\"][1] + 1),\n",
        "    (0, base_config[\"z(km)\"][1] + 1),\n",
        "    (None, None),\n",
        ")\n",
        "\n",
        "# Process windows with velocity grid search\n",
        "all_catalogs = []\n",
        "all_assignments = []\n",
        "results = []\n",
        "\n",
        "start_processing = datetime.now()\n",
        "\n",
        "for win_idx, window in enumerate(windows):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Window {win_idx+1}/{len(windows)}: {window['start']} ({window['n_picks']} picks)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Extract picks for this window\n",
        "    mask = (pick_df['timestamp'] >= window['start']) & (pick_df['timestamp'] < window['end'])\n",
        "    window_picks = pick_df[mask].reset_index(drop=True)\n",
        "\n",
        "    best_n_events = 0\n",
        "    best_catalog = None\n",
        "    best_assignments = None\n",
        "    best_params = None\n",
        "\n",
        "    # Grid search over velocities\n",
        "    for vp in vel_p_values:\n",
        "        for vs in vel_s_values:\n",
        "            if vs >= vp:  # Skip invalid Vs >= Vp\n",
        "                continue\n",
        "\n",
        "            config = base_config.copy()\n",
        "            config[\"vel\"] = {\"p\": vp, \"s\": vs}\n",
        "\n",
        "            try:\n",
        "                catalogs, assignments = association(window_picks, station_df, config, method=config[\"method\"])\n",
        "                n_events = len(catalogs)\n",
        "\n",
        "                if n_events > best_n_events:\n",
        "                    best_n_events = n_events\n",
        "                    best_catalog = catalogs\n",
        "                    best_assignments = assignments\n",
        "                    best_params = {'vp': vp, 'vs': vs}\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    # Save best result for this window\n",
        "    if best_n_events > 0:\n",
        "        catalog_df = pd.DataFrame(best_catalog)\n",
        "        catalog_df['window_idx'] = win_idx\n",
        "        catalog_df['window_start'] = window['start']\n",
        "\n",
        "        assignments_df = pd.DataFrame(best_assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\n",
        "        assignments_df['window_idx'] = win_idx\n",
        "\n",
        "        all_catalogs.append(catalog_df)\n",
        "        all_assignments.append(assignments_df)\n",
        "\n",
        "        avg_picks = assignments_df.groupby('event_index').size().mean()\n",
        "        print(f\"\\n✓ Found {best_n_events} events (Vp={best_params['vp']}, Vs={best_params['vs']:.1f})\")\n",
        "        print(f\"  Avg picks per event: {avg_picks:.1f}\")\n",
        "\n",
        "        results.append({\n",
        "            'window_idx': win_idx,\n",
        "            'window_start': window['start'],\n",
        "            'n_events': best_n_events,\n",
        "            'n_picks': window['n_picks'],\n",
        "            'vp': best_params['vp'],\n",
        "            'vs': best_params['vs']\n",
        "        })\n",
        "    else:\n",
        "        print(f\"\\n✗ No events found in this window\")\n",
        "\n",
        "# ============================================================\n",
        "# COMBINE AND SAVE RESULTS\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMBINING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if all_catalogs:\n",
        "    final_catalog = pd.concat(all_catalogs, ignore_index=True)\n",
        "    final_assignments = pd.concat(all_assignments, ignore_index=True)\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    final_catalog.to_csv(\"/kaggle/working/catalog_windowed(15).csv\", index=False)\n",
        "    final_assignments.to_csv(\"/kaggle/working/assignments_windowed(15).csv\", index=False)\n",
        "    results_df.to_csv(\"/kaggle/working/window_results(15).csv\", index=False)\n",
        "\n",
        "    print(f\"\\n✓ Total events detected: {len(final_catalog)}\")\n",
        "    print(f\"✓ Total assignments: {len(final_assignments)}\")\n",
        "    print(f\"✓ Windows with events: {len(results_df)}/{len(windows)}\")\n",
        "    print(f\"\\n✓ Catalog saved to: catalog_windowed.csv\")\n",
        "    print(f\"✓ Assignments saved to: assignments_windowed.csv\")\n",
        "    print(f\"✓ Window summary saved to: window_results.csv\")\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nTotal events: {len(final_catalog)}\")\n",
        "    print(f\"Events per window (avg): {results_df['n_events'].mean():.1f}\")\n",
        "    print(f\"Most common Vp: {results_df['vp'].mode().values[0]} km/s\")\n",
        "    print(f\"Most common Vs: {results_df['vs'].mode().values[0]:.1f} km/s\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠ No events detected in any window!\")\n",
        "\n",
        "elapsed = (datetime.now() - start_processing).total_seconds()\n",
        "print(f\"\\n⏱ Total processing time: {elapsed/60:.1f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Earthquake Event Analysis & Classification\n",
        "\n",
        "## Overview\n",
        "This script analyzes the detected earthquake catalog from GAMMA and classifies events based on quality, spatial clustering, and temporal patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Quality-Based Classification\n",
        "\n",
        "Events are categorized into 4 quality levels based on number of picks and gamma score:\n",
        "\n",
        "```python\n",
        "HIGH_QUALITY    = (num_picks >= 10) & (gamma_score >= 15)\n",
        "MEDIUM_QUALITY  = (num_picks >= 5) & (gamma_score >= 5)\n",
        "LOW_QUALITY     = (num_picks >= 3)\n",
        "NOISE           = (num_picks < 3)\n",
        "```\n",
        "\n",
        "**Rationale:**\n",
        "- More picks = better location constraint\n",
        "- Higher gamma score = stronger association confidence\n",
        "- Events with <3 picks are likely false detections\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Spatial Clustering (DBSCAN)\n",
        "\n",
        "Groups events that are spatially close to identify seismic zones:\n",
        "\n",
        "```python\n",
        "DBSCAN(eps=20, min_samples=2)\n",
        "```\n",
        "\n",
        "**Parameters:**\n",
        "- `eps=20 km`: Events within 20km radius are considered neighbors\n",
        "- `min_samples=2`: At least 2 events needed to form a cluster\n",
        "- Isolated events labeled as `-1` (noise)\n",
        "\n",
        "**Output:** Identifies active seismic zones and isolated events\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Temporal Clustering\n",
        "\n",
        "Identifies sequences of events happening close in time:\n",
        "\n",
        "```python\n",
        "temporal_sequence = (time_diff > 30 minutes).cumsum()\n",
        "```\n",
        "\n",
        "**Logic:**\n",
        "- Calculates time gap between consecutive events\n",
        "- Gaps >30 minutes define new sequences\n",
        "- Useful for identifying aftershock sequences or swarms\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Statistics by Quality\n",
        "\n",
        "Computes average metrics for each quality category:\n",
        "- Number of picks (P + S waves)\n",
        "- Gamma association score\n",
        "- Depth and depth range\n",
        "- Spatial distribution\n",
        "\n",
        "---\n",
        "\n",
        "## Output Files\n",
        "\n",
        "### `catalog_classified.csv`\n",
        "Enhanced catalog with additional columns:\n",
        "- `quality`: HIGH/MEDIUM/LOW/NOISE\n",
        "- `spatial_cluster`: Cluster ID or -1 for isolated\n",
        "- `temporal_sequence`: Sequence ID\n",
        "- `time_diff_minutes`: Gap since previous event\n",
        "\n",
        "---\n",
        "\n",
        "## Visualization (9-Panel Plot)\n",
        "\n",
        "### Row 1: Spatial Analysis\n",
        "1. **Event Map by Quality**: Color-coded by quality (red=high, gray=noise)\n",
        "2. **Depth Distribution**: Histogram of event depths per quality level\n",
        "3. **Spatial Clusters**: Map showing DBSCAN clusters\n",
        "\n",
        "### Row 2: Event Characteristics\n",
        "4. **Pick Distribution**: Histogram of number of picks per event\n",
        "5. **Gamma Score Distribution**: Quality metric distribution\n",
        "6. **Event Timeline**: Events per day over time\n",
        "\n",
        "### Row 3: Cross-Sections & Relationships\n",
        "7. **X-Z Cross-Section**: Vertical slice showing depth profile\n",
        "8. **P vs S Picks**: Scatter plot (colored by gamma score)\n",
        "9. **Quality Pie Chart**: Percentage breakdown of quality levels\n",
        "\n",
        "---\n",
        "\n",
        "## 3D Visualization\n",
        "\n",
        "Additional 3D plots showing:\n",
        "- **Events by Quality**: 3D spatial distribution colored by quality\n",
        "- **Spatial Clusters**: 3D view of DBSCAN clusters\n",
        "- **Events by Metrics**: Size = number of picks, Color = gamma score\n",
        "\n",
        "---\n",
        "\n",
        "## Key Insights from Analysis\n",
        "\n",
        "### Quality Distribution\n",
        "Shows what percentage of detections are reliable:\n",
        "- High quality → Well-constrained, trustworthy locations\n",
        "- Noise → Should be filtered out before further analysis\n",
        "\n",
        "### Spatial Clustering\n",
        "Reveals:\n",
        "- Active fault zones (large clusters)\n",
        "- Isolated events (possible triggered events)\n",
        "- Spatial extent of seismicity\n",
        "\n",
        "### Temporal Patterns\n",
        "Identifies:\n",
        "- Mainshock-aftershock sequences\n",
        "- Earthquake swarms (sustained activity)\n",
        "- Background seismicity vs. sequences\n",
        "\n",
        "---\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "# Load and run analysis\n",
        "catalog = pd.read_csv(\"catalog_windowed(60).csv\")\n",
        "# ... run classification and clustering ...\n",
        "\n",
        "# Filter high-quality events only\n",
        "high_quality = catalog[catalog['quality'] == 'HIGH']\n",
        "\n",
        "# Analyze specific cluster\n",
        "cluster_0 = catalog[catalog['spatial_cluster'] == 0]\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YWC5X3dhbUDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the catalog\n",
        "catalog = pd.read_csv(\"/kaggle/working/catalog_windowed(60).csv\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EARTHQUAKE EVENT ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"\\nTotal detected events: {len(catalog)}\")\n",
        "print(f\"Unique windows: {catalog['window_idx'].nunique()}\")\n",
        "print(f\"Date range: {catalog['time'].min()} to {catalog['time'].max()}\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. QUALITY-BASED CLASSIFICATION\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVENT QUALITY CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define quality thresholds\n",
        "HIGH_QUALITY = (catalog['num_picks'] >= 10) & (catalog['gamma_score'] >= 15)\n",
        "MEDIUM_QUALITY = (catalog['num_picks'] >= 5) & (catalog['gamma_score'] >= 5) & (~HIGH_QUALITY)\n",
        "LOW_QUALITY = (catalog['num_picks'] >= 3) & (~HIGH_QUALITY) & (~MEDIUM_QUALITY)\n",
        "NOISE = catalog['num_picks'] < 3\n",
        "\n",
        "catalog['quality'] = 'NOISE'\n",
        "catalog.loc[LOW_QUALITY, 'quality'] = 'LOW'\n",
        "catalog.loc[MEDIUM_QUALITY, 'quality'] = 'MEDIUM'\n",
        "catalog.loc[HIGH_QUALITY, 'quality'] = 'HIGH'\n",
        "\n",
        "print(\"\\nQuality Distribution:\")\n",
        "quality_counts = catalog['quality'].value_counts()\n",
        "for quality, count in quality_counts.items():\n",
        "    pct = 100 * count / len(catalog)\n",
        "    print(f\"  {quality:8s}: {count:3d} events ({pct:5.1f}%)\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. SPATIAL CLUSTERING\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SPATIAL CLUSTERING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Extract coordinates\n",
        "coords = catalog[['x(km)', 'y(km)', 'z(km)']].values\n",
        "\n",
        "# DBSCAN clustering (eps in km)\n",
        "dbscan = DBSCAN(eps=20, min_samples=2)  # 20km radius\n",
        "catalog['spatial_cluster'] = dbscan.fit_predict(coords)\n",
        "\n",
        "n_clusters = len(set(catalog['spatial_cluster'])) - (1 if -1 in catalog['spatial_cluster'] else 0)\n",
        "n_noise = (catalog['spatial_cluster'] == -1).sum()\n",
        "\n",
        "print(f\"\\nSpatial clusters found: {n_clusters}\")\n",
        "print(f\"Isolated events (noise): {n_noise}\")\n",
        "print(f\"Clustered events: {len(catalog) - n_noise}\")\n",
        "\n",
        "# Cluster statistics\n",
        "if n_clusters > 0:\n",
        "    print(\"\\nTop 5 Largest Clusters:\")\n",
        "    cluster_sizes = catalog[catalog['spatial_cluster'] != -1]['spatial_cluster'].value_counts()\n",
        "    for i, (cluster_id, size) in enumerate(cluster_sizes.head(5).items(), 1):\n",
        "        cluster_events = catalog[catalog['spatial_cluster'] == cluster_id]\n",
        "        center_x = cluster_events['x(km)'].mean()\n",
        "        center_y = cluster_events['y(km)'].mean()\n",
        "        center_z = cluster_events['z(km)'].mean()\n",
        "        print(f\"  Cluster {cluster_id}: {size} events at ({center_x:.1f}, {center_y:.1f}, {center_z:.1f}) km\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. TEMPORAL CLUSTERING\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEMPORAL CLUSTERING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "catalog['time'] = pd.to_datetime(catalog['time'])\n",
        "catalog = catalog.sort_values('time').reset_index(drop=True)\n",
        "\n",
        "# Calculate time differences (in minutes)\n",
        "time_diffs = catalog['time'].diff().dt.total_seconds() / 60\n",
        "catalog['time_diff_minutes'] = time_diffs\n",
        "\n",
        "# Find temporal sequences (events within 30 minutes)\n",
        "catalog['temporal_sequence'] = (time_diffs > 30).cumsum()\n",
        "\n",
        "n_sequences = catalog['temporal_sequence'].nunique()\n",
        "print(f\"\\nTemporal sequences (>30 min gaps): {n_sequences}\")\n",
        "\n",
        "sequence_sizes = catalog['temporal_sequence'].value_counts().head(5)\n",
        "print(\"\\nTop 5 Most Active Sequences:\")\n",
        "for seq_id, size in sequence_sizes.items():\n",
        "    seq_events = catalog[catalog['temporal_sequence'] == seq_id]\n",
        "    duration = (seq_events['time'].max() - seq_events['time'].min()).total_seconds() / 60\n",
        "    print(f\"  Sequence {seq_id}: {size} events over {duration:.1f} minutes\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. STATISTICS BY QUALITY\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTICS BY QUALITY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for quality in ['HIGH', 'MEDIUM', 'LOW', 'NOISE']:\n",
        "    subset = catalog[catalog['quality'] == quality]\n",
        "    if len(subset) > 0:\n",
        "        print(f\"\\n{quality} QUALITY EVENTS ({len(subset)}):\")\n",
        "        print(f\"  Avg picks: {subset['num_picks'].mean():.1f}\")\n",
        "        print(f\"  Avg gamma score: {subset['gamma_score'].mean():.1f}\")\n",
        "        print(f\"  Avg depth: {subset['z(km)'].mean():.1f} km\")\n",
        "        print(f\"  Depth range: {subset['z(km)'].min():.1f} - {subset['z(km)'].max():.1f} km\")\n",
        "\n",
        "# ============================================================\n",
        "# SAVE CLASSIFIED DATA\n",
        "# ============================================================\n",
        "catalog.to_csv(\"/kaggle/working/catalog_classified.csv\", index=False)\n",
        "print(\"\\n✓ Classified catalog saved to: catalog_classified.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# PLOTTING\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATING PLOTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fig = plt.figure(figsize=(20, 14))\n",
        "\n",
        "# Plot 1: Map view with quality colors\n",
        "ax1 = plt.subplot(3, 3, 1)\n",
        "colors = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'yellow', 'NOISE': 'gray'}\n",
        "for quality in ['NOISE', 'LOW', 'MEDIUM', 'HIGH']:\n",
        "    subset = catalog[catalog['quality'] == quality]\n",
        "    ax1.scatter(subset['x(km)'], subset['y(km)'],\n",
        "               c=colors[quality], s=50, alpha=0.6, label=quality, edgecolors='black', linewidth=0.5)\n",
        "ax1.set_xlabel('X (km)', fontsize=10)\n",
        "ax1.set_ylabel('Y (km)', fontsize=10)\n",
        "ax1.set_title('Event Map by Quality', fontsize=12, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Depth distribution by quality\n",
        "ax2 = plt.subplot(3, 3, 2)\n",
        "for quality in ['HIGH', 'MEDIUM', 'LOW', 'NOISE']:\n",
        "    subset = catalog[catalog['quality'] == quality]\n",
        "    if len(subset) > 0:\n",
        "        ax2.hist(subset['z(km)'], bins=20, alpha=0.5, label=quality, color=colors[quality])\n",
        "ax2.set_xlabel('Depth (km)', fontsize=10)\n",
        "ax2.set_ylabel('Count', fontsize=10)\n",
        "ax2.set_title('Depth Distribution by Quality', fontsize=12, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Spatial clusters\n",
        "ax3 = plt.subplot(3, 3, 3)\n",
        "unique_clusters = catalog[catalog['spatial_cluster'] != -1]['spatial_cluster'].unique()\n",
        "cmap = plt.cm.get_cmap('tab20', len(unique_clusters))\n",
        "for i, cluster_id in enumerate(unique_clusters):\n",
        "    subset = catalog[catalog['spatial_cluster'] == cluster_id]\n",
        "    ax3.scatter(subset['x(km)'], subset['y(km)'],\n",
        "               c=[cmap(i)], s=100, alpha=0.7, label=f'C{cluster_id}', edgecolors='black', linewidth=0.5)\n",
        "# Plot noise\n",
        "noise = catalog[catalog['spatial_cluster'] == -1]\n",
        "ax3.scatter(noise['x(km)'], noise['y(km)'],\n",
        "           c='lightgray', s=30, alpha=0.5, label='Isolated', edgecolors='black', linewidth=0.3)\n",
        "ax3.set_xlabel('X (km)', fontsize=10)\n",
        "ax3.set_ylabel('Y (km)', fontsize=10)\n",
        "ax3.set_title(f'Spatial Clusters (n={n_clusters})', fontsize=12, fontweight='bold')\n",
        "ax3.legend(ncol=2, fontsize=8)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Number of picks distribution\n",
        "ax4 = plt.subplot(3, 3, 4)\n",
        "ax4.hist(catalog['num_picks'], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "ax4.axvline(catalog['num_picks'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {catalog[\"num_picks\"].mean():.1f}')\n",
        "ax4.set_xlabel('Number of Picks', fontsize=10)\n",
        "ax4.set_ylabel('Count', fontsize=10)\n",
        "ax4.set_title('Pick Distribution', fontsize=12, fontweight='bold')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Gamma score distribution\n",
        "ax5 = plt.subplot(3, 3, 5)\n",
        "ax5.hist(catalog['gamma_score'], bins=30, color='darkgreen', alpha=0.7, edgecolor='black')\n",
        "ax5.axvline(catalog['gamma_score'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {catalog[\"gamma_score\"].mean():.1f}')\n",
        "ax5.set_xlabel('Gamma Score', fontsize=10)\n",
        "ax5.set_ylabel('Count', fontsize=10)\n",
        "ax5.set_title('Gamma Score Distribution', fontsize=12, fontweight='bold')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Timeline\n",
        "ax6 = plt.subplot(3, 3, 6)\n",
        "catalog['date'] = catalog['time'].dt.date\n",
        "events_per_day = catalog.groupby('date').size()\n",
        "ax6.plot(events_per_day.index, events_per_day.values, marker='o', linewidth=2, markersize=6, color='purple')\n",
        "ax6.set_xlabel('Date', fontsize=10)\n",
        "ax6.set_ylabel('Events per Day', fontsize=10)\n",
        "ax6.set_title('Event Timeline', fontsize=12, fontweight='bold')\n",
        "ax6.tick_params(axis='x', rotation=45)\n",
        "ax6.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 7: Cross-section (X-Z)\n",
        "ax7 = plt.subplot(3, 3, 7)\n",
        "for quality in ['NOISE', 'LOW', 'MEDIUM', 'HIGH']:\n",
        "    subset = catalog[catalog['quality'] == quality]\n",
        "    ax7.scatter(subset['x(km)'], subset['z(km)'],\n",
        "               c=colors[quality], s=50, alpha=0.6, label=quality, edgecolors='black', linewidth=0.5)\n",
        "ax7.set_xlabel('X (km)', fontsize=10)\n",
        "ax7.set_ylabel('Depth (km)', fontsize=10)\n",
        "ax7.set_title('Cross-Section (X-Z)', fontsize=12, fontweight='bold')\n",
        "ax7.invert_yaxis()\n",
        "ax7.legend()\n",
        "ax7.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 8: P vs S picks\n",
        "ax8 = plt.subplot(3, 3, 8)\n",
        "ax8.scatter(catalog['num_p_picks'], catalog['num_s_picks'],\n",
        "           c=catalog['gamma_score'], s=80, alpha=0.6, cmap='viridis', edgecolors='black', linewidth=0.5)\n",
        "ax8.plot([0, catalog['num_p_picks'].max()], [0, catalog['num_p_picks'].max()],\n",
        "         'r--', linewidth=2, label='P=S line')\n",
        "ax8.set_xlabel('P Picks', fontsize=10)\n",
        "ax8.set_ylabel('S Picks', fontsize=10)\n",
        "ax8.set_title('P vs S Picks (color=gamma score)', fontsize=12, fontweight='bold')\n",
        "ax8.legend()\n",
        "ax8.grid(True, alpha=0.3)\n",
        "cbar = plt.colorbar(ax8.collections[0], ax=ax8)\n",
        "cbar.set_label('Gamma Score', fontsize=9)\n",
        "\n",
        "# Plot 9: Quality pie chart\n",
        "ax9 = plt.subplot(3, 3, 9)\n",
        "quality_counts = catalog['quality'].value_counts()\n",
        "ax9.pie(quality_counts.values, labels=quality_counts.index, autopct='%1.1f%%',\n",
        "        colors=[colors[q] for q in quality_counts.index], startangle=90)\n",
        "ax9.set_title('Event Quality Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/earthquake_analysis.png', dpi=150, bbox_inches='tight')\n",
        "print(\"✓ Plot saved to: earthquake_analysis.png\")\n",
        "\n",
        "# ============================================================\n",
        "# ADDITIONAL DETAILED PLOTS\n",
        "# ============================================================\n",
        "\n",
        "# 3D plot if mpl_toolkits available\n",
        "try:\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "    fig2 = plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # 3D plot by quality\n",
        "    ax = fig2.add_subplot(131, projection='3d')\n",
        "    for quality in ['NOISE', 'LOW', 'MEDIUM', 'HIGH']:\n",
        "        subset = catalog[catalog['quality'] == quality]\n",
        "        ax.scatter(subset['x(km)'], subset['y(km)'], subset['z(km)'],\n",
        "                  c=colors[quality], s=50, alpha=0.6, label=quality)\n",
        "    ax.set_xlabel('X (km)')\n",
        "    ax.set_ylabel('Y (km)')\n",
        "    ax.set_zlabel('Depth (km)')\n",
        "    ax.set_title('3D Event Distribution by Quality')\n",
        "    ax.legend()\n",
        "    ax.invert_zaxis()\n",
        "\n",
        "    # 3D plot by cluster\n",
        "    ax2 = fig2.add_subplot(132, projection='3d')\n",
        "    for i, cluster_id in enumerate(unique_clusters[:10]):  # Top 10 clusters\n",
        "        subset = catalog[catalog['spatial_cluster'] == cluster_id]\n",
        "        ax2.scatter(subset['x(km)'], subset['y(km)'], subset['z(km)'],\n",
        "                   s=100, alpha=0.7, label=f'C{cluster_id}')\n",
        "    noise = catalog[catalog['spatial_cluster'] == -1]\n",
        "    ax2.scatter(noise['x(km)'], noise['y(km)'], noise['z(km)'],\n",
        "               c='lightgray', s=20, alpha=0.3, label='Isolated')\n",
        "    ax2.set_xlabel('X (km)')\n",
        "    ax2.set_ylabel('Y (km)')\n",
        "    ax2.set_zlabel('Depth (km)')\n",
        "    ax2.set_title('3D Spatial Clusters')\n",
        "    ax2.legend(ncol=2)\n",
        "    ax2.invert_zaxis()\n",
        "\n",
        "    # 3D plot sized by gamma score\n",
        "    ax3 = fig2.add_subplot(133, projection='3d')\n",
        "    scatter = ax3.scatter(catalog['x(km)'], catalog['y(km)'], catalog['z(km)'],\n",
        "                         c=catalog['gamma_score'], s=catalog['num_picks']*10,\n",
        "                         alpha=0.6, cmap='plasma')\n",
        "    ax3.set_xlabel('X (km)')\n",
        "    ax3.set_ylabel('Y (km)')\n",
        "    ax3.set_zlabel('Depth (km)')\n",
        "    ax3.set_title('Events (size=picks, color=gamma)')\n",
        "    ax3.invert_zaxis()\n",
        "    plt.colorbar(scatter, ax=ax3, label='Gamma Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/kaggle/working/earthquake_3d_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"✓ 3D plot saved to: earthquake_3d_analysis.png\")\n",
        "except:\n",
        "    print(\"⚠ 3D plotting not available\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Total events: {len(catalog)}\")\n",
        "print(f\"  High quality: {(catalog['quality'] == 'HIGH').sum()}\")\n",
        "print(f\"  Medium quality: {(catalog['quality'] == 'MEDIUM').sum()}\")\n",
        "print(f\"  Low quality: {(catalog['quality'] == 'LOW').sum()}\")\n",
        "print(f\"  Noise: {(catalog['quality'] == 'NOISE').sum()}\")\n",
        "print(f\"  Spatial clusters: {n_clusters}\")\n",
        "print(f\"  Temporal sequences: {n_sequences}\")"
      ],
      "metadata": {
        "id": "iui30LdLYJpb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}